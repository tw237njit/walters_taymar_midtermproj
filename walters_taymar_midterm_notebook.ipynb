{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LW9S-8i0ZbE"
      },
      "source": [
        "# CS634 – Data Mining Midterm Project\n",
        "### Author: Taymar Walters\n",
        "\n",
        "This notebook is a breakdown on the code execution for my data mining project using:\n",
        "- A **Brute Force implementation (from scratch)**\n",
        "- **Apriori** (mlxtend)\n",
        "- **FP-Growth** (mlxtend)\n",
        "\n",
        "It shows:\n",
        "1. Dataset loading\n",
        "2. Algorithm execution\n",
        "3. Frequent itemsets and rules\n",
        "\n"
      ],
      "id": "7LW9S-8i0ZbE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvHwLW_50ZbF"
      },
      "source": [
        "## Import Packages\n",
        "This is part of the code will install all the needed packages includings the ones that are missing."
      ],
      "id": "XvHwLW_50ZbF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGgbYJsC0ZbG"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "def install_if_missing(package):\n",
        "    try:\n",
        "        __import__(package)\n",
        "    except ImportError:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "\n",
        "for pkg in [\"pandas\", \"mlxtend\"]:\n",
        "    install_if_missing(pkg)\n",
        "\n",
        "from mlxtend.frequent_patterns import apriori, fpgrowth, association_rules\n",
        "from mlxtend.preprocessing import TransactionEncoder"
      ],
      "id": "MGgbYJsC0ZbG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKE4VENC0ZbG"
      },
      "source": [
        "## Load and Preview Dataset\n",
        "Here we ask the user to select a transactional dataset and the unique list of items corressponding to that dataset. All of them are saved as CSV files. For this execution, we will use `generic_transactions.csv` and the corresponding `generic_items.csv` file for demonstration."
      ],
      "id": "HKE4VENC0ZbG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eih7OSFl0ZbH"
      },
      "outputs": [],
      "source": [
        "# Prompting the User to select a dataset\n",
        "print(\"Here are the following transactional databases\\n\"\n",
        "      \" 1) Generic\\n 2) Nike\\n 3) Best Buy\\n 4) Coffee Shop\\n 5) K-mart\\n \")\n",
        "items = \"\"\n",
        "def selectfile():\n",
        "    while True:\n",
        "        try:\n",
        "            fileNumber = int(input(\"Enter number to select a database: \\n\"))\n",
        "            match fileNumber:\n",
        "                case 1:\n",
        "                    items = \"generic_items.csv\"\n",
        "                    return \"generic_transactions.csv\", items\n",
        "                case 2:\n",
        "                    items = \"nike_products.csv\"\n",
        "                    return \"nike_product_transactions.csv\", items\n",
        "                case 3:\n",
        "                    items = \"bestbuy_products.csv\"\n",
        "                    return \"bestbuy_transactions.csv\", items\n",
        "                case 4:\n",
        "                    items = \"coffee_items.csv\"\n",
        "                    return \"coffee_transactions.csv\", items\n",
        "                case 5:\n",
        "                    items = \"k-mart_items.csv\"\n",
        "                    return \"k-mart_transactions.csv\", items\n",
        "                case _:\n",
        "                    print(\"Invalid input. Please try again.\")\n",
        "        except ValueError:\n",
        "            print(\"Please enter a valid number between 1–5.\")\n",
        "# Get file path for the individual items dataset\n",
        "transactions, items = selectfile()\n",
        "base_path = os.path.dirname(os.path.abspath(__file__))\n",
        "file_path = os.path.join(base_path, items)\n",
        "# print the list of seperate items\n",
        "print(\"=============================================================\")\n",
        "print(\"Here are the unique items corresponding to the transactions:\")\n",
        "print(\"=============================================================\")\n",
        "df = pd.read_csv(file_path)\n",
        "df = df.dropna(how='all')\n",
        "df.columns = df.columns.str.strip()\n",
        "df[\"Item #\"] = df[\"Item #\"].astype(int)\n",
        "print(df.to_string(index=False))\n",
        "print(\"================================================\")\n",
        "# Get file path for the transaction dataset\n",
        "base_path = os.path.dirname(os.path.abspath(__file__))\n",
        "file_path = os.path.join(base_path, transactions)\n",
        "# Columns that might represent transactions\n",
        "possible_cols = [\"transaction\", \"transactions\", \"items\", \"basket\"]\n",
        "\n",
        "try:\n",
        "    # Try to read the CSV normally, then fallback to alternate delimiter\n",
        "    try:\n",
        "        data = pd.read_csv(file_path)\n",
        "    except Exception:\n",
        "        data = pd.read_csv(file_path, delimiter=';')\n",
        "\n",
        "    data.columns = data.columns.str.strip().str.lower()\n",
        "\n",
        "    # Detect transaction column dynamically\n",
        "    target_col = None\n",
        "    for col in possible_cols:\n",
        "        if col in data.columns:\n",
        "            target_col = col\n",
        "            break\n",
        "\n",
        "    if target_col is None:\n",
        "        raise KeyError(\"❌ No valid transaction column found in this file.\")\n",
        "\n",
        "    transactions = [\n",
        "        str(t).replace(\" \", \"\").split(\",\")\n",
        "        for t in data[target_col]\n",
        "        if pd.notna(t)\n",
        "    ]\n",
        "\n",
        "except FileNotFoundError:\n",
        "    raise FileNotFoundError(f\"❌ File not found: {file_path}\")\n",
        "except Exception as e:\n",
        "    raise RuntimeError(f\"⚠️ Error loading data: {e}\")\n",
        "\n",
        "all_items = sorted(set(item for sublist in transactions for item in sublist))"
      ],
      "id": "eih7OSFl0ZbH"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDQJAL5C0ZbH"
      },
      "source": [
        "## User Input for Support & Confidence\n",
        "This is where the program will now ask the user to enter a value for both the minimum support and confidence while checking for valid inputs.\n",
        "\n",
        "**Default Parameters:**\n",
        "\n",
        "*Minimum Support* = 0.3\n",
        "\n",
        "*Minimum Confidence* = 0.6"
      ],
      "id": "jDQJAL5C0ZbH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMf90yw00ZbH"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    min_support = float(input(\"Enter minimum support (e.g., 0.3 for 30%): \"))\n",
        "except ValueError:\n",
        "    print(\"Invalid input. Using default min_support = 0.3\")\n",
        "    min_support = 0.3\n",
        "\n",
        "try:\n",
        "    min_confidence = float(input(\"Enter minimum confidence (e.g., 0.6 for 60%): \"))\n",
        "except ValueError:\n",
        "    print(\"Invalid input. Using default min_confidence = 0.6\")\n",
        "    min_confidence = 0.6\n",
        "\n",
        "print(f\"\\nUsing min_support = {min_support} and min_confidence = {min_confidence}\\n\")"
      ],
      "id": "EMf90yw00ZbH"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjYEUx7P0ZbH"
      },
      "source": [
        "## Running Brute Force for frequent itemset mining"
      ],
      "id": "CjYEUx7P0ZbH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F61U309i0ZbI"
      },
      "outputs": [],
      "source": [
        "def get_support(itemset, transactions):\n",
        "    \"\"\"Compute support count for a given itemset.\"\"\"\n",
        "    return sum(1 for t in transactions if set(itemset).issubset(set(t)))\n",
        "\n",
        "def brute_force_mining(transactions, min_support):\n",
        "    num_transactions = len(transactions)\n",
        "    frequent_itemsets = []\n",
        "    k = 1\n",
        "\n",
        "    while True:\n",
        "        candidates = [list(i) for i in itertools.combinations(all_items, k)]\n",
        "        level_frequent = []\n",
        "        for c in candidates:\n",
        "            support = get_support(c, transactions) / num_transactions\n",
        "            if support >= min_support:\n",
        "                level_frequent.append((tuple(c), support))\n",
        "\n",
        "        if not level_frequent:\n",
        "            break\n",
        "\n",
        "        frequent_itemsets.extend(level_frequent)\n",
        "        print(f\"Found {len(level_frequent)} frequent {k}-itemsets\")\n",
        "        k += 1\n",
        "\n",
        "    return frequent_itemsets\n",
        "\n"
      ],
      "id": "F61U309i0ZbI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Association Rules from Brute Force"
      ],
      "metadata": {
        "id": "UIY07QCr9iI_"
      },
      "id": "UIY07QCr9iI_"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nRunning Brute-Force Algorithm...\")\n",
        "start_brute = time.time()\n",
        "frequent_itemsets_brute = brute_force_mining(transactions, min_support)\n",
        "rules_brute = []\n",
        "rules_brute_start = time.time()\n",
        "\n",
        "def generate_rules(frequent_itemsets, min_confidence, transactions):\n",
        "    num_transactions = len(transactions)\n",
        "    rules = []\n",
        "    for itemset, support in frequent_itemsets:\n",
        "        if len(itemset) < 2:\n",
        "            continue\n",
        "        for i in range(1, len(itemset)):\n",
        "            for antecedent in itertools.combinations(itemset, i):\n",
        "                consequent = tuple(set(itemset) - set(antecedent))\n",
        "                sup_itemset = get_support(itemset, transactions) / num_transactions\n",
        "                sup_antecedent = get_support(antecedent, transactions) / num_transactions\n",
        "                confidence = sup_itemset / sup_antecedent if sup_antecedent > 0 else 0\n",
        "                if confidence >= min_confidence:\n",
        "                    rules.append((antecedent, consequent, sup_itemset, confidence))\n",
        "    return rules\n",
        "\n",
        "rules_brute = generate_rules(frequent_itemsets_brute, min_confidence, transactions)\n",
        "end_brute = time.time()\n",
        "brute_force_time = end_brute - start_brute\n",
        "rules_brute = generate_rules(frequent_itemsets_brute, min_confidence, transactions)\n"
      ],
      "metadata": {
        "id": "VbNJm78J9pHQ"
      },
      "id": "VbNJm78J9pHQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xu6tvcP0ZbI"
      },
      "source": [
        "## Apriori and FP-Growth Execution\n"
      ],
      "id": "3xu6tvcP0ZbI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVqHFN9e0ZbI"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
        "\n",
        "print(\"\\nRunning Apriori Algorithm...\")\n",
        "start_apriori = time.time()\n",
        "frequent_itemsets_ap = apriori(one_hot, min_support=min_support, use_colnames=True)\n",
        "rules_ap = association_rules(frequent_itemsets_ap, metric=\"confidence\", min_threshold=min_confidence)\n",
        "rules_ap = rules_ap.dropna()\n",
        "rules_ap = rules_ap[(rules_ap['support'] > 0) & (rules_ap['confidence'] > 0)]\n",
        "end_apriori = time.time()\n",
        "apriori_time = end_apriori - start_apriori\n",
        "\n",
        "print(\"Running FP-Growth Algorithm...\")\n",
        "start_fp = time.time()\n",
        "frequent_itemsets_fp = fpgrowth(one_hot, min_support=min_support, use_colnames=True)\n",
        "rules_fp = association_rules(frequent_itemsets_fp, metric=\"confidence\", min_threshold=min_confidence)\n",
        "rules_fp = rules_fp.dropna()\n",
        "rules_fp = rules_fp[(rules_fp['support'] > 0) & (rules_fp['confidence'] > 0)]\n",
        "end_fp = time.time()\n",
        "fp_growth_time = end_fp - start_fp"
      ],
      "id": "mVqHFN9e0ZbI"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfjWEFiM0ZbI"
      },
      "source": [
        "## Generating and Displaying Association Rules with ALL 3 Algorithms."
      ],
      "id": "wfjWEFiM0ZbI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PqScau5j0ZbI"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\\n================================================\")\n",
        "print(\"🔹 FREQUENT ITEMS FOUND BY BRUTE FORCE:\")\n",
        "print(\"================================================\")\n",
        "for items, sup in frequent_itemsets_brute:\n",
        "    print(f\"{items} | support: {sup:.2f}\")\n",
        "\n",
        "print(\"\\n\\n================================================\")\n",
        "print(\"🔸 ASSOCIATION RULES — BRUTE FORCE\")\n",
        "print(\"================================================\")\n",
        "for ant, cons, sup, conf in rules_brute:\n",
        "    print(f\"{ant} → {cons} (support: {sup:.2f}, confidence: {conf:.2f})\")\n",
        "\n",
        "print(\"\\n\\n================================================\")\n",
        "print(\"🔸 ASSOCIATION RULES — APRIORI\")\n",
        "print(\"================================================\")\n",
        "for _, row in rules_ap.iterrows():\n",
        "    print(f\"{tuple(row['antecedents'])} → {tuple(row['consequents'])} \"\n",
        "          f\"(support: {row['support']:.2f}, confidence: {row['confidence']:.2f})\")\n",
        "\n",
        "print(\"\\n\\n================================================\")\n",
        "print(\"🔸 ASSOCIATION RULES — FP-GROWTH\")\n",
        "print(\"================================================\")\n",
        "for _, row in rules_fp.iterrows():\n",
        "    print(f\"{tuple(row['antecedents'])} → {tuple(row['consequents'])} \"\n",
        "          f\"(support: {row['support']:.2f}, confidence: {row['confidence']:.2f})\")\n"
      ],
      "id": "PqScau5j0ZbI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Displaying Timing Summary"
      ],
      "metadata": {
        "id": "cxLPuaumNXs8"
      },
      "id": "cxLPuaumNXs8"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\\n================================================\")\n",
        "print(\"⏱️ EXECUTION TIME SUMMARY (seconds)\")\n",
        "print(\"================================================\")\n",
        "print(f\"Brute-Force Algorithm: {brute_force_time:.4f} sec\")\n",
        "print(f\"Apriori Algorithm:     {apriori_time:.4f} sec\")\n",
        "print(f\"FP-Growth Algorithm:   {fp_growth_time:.4f} sec\")\n",
        "print(\"================================================\")\n",
        "\n",
        "print(\"\\n✅ All algorithms executed successfully!\")"
      ],
      "metadata": {
        "id": "QGfE9_AzNTd0"
      },
      "id": "QGfE9_AzNTd0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfyRXJc_0ZbJ"
      },
      "source": [
        "##OUTPUT:\n",
        "This is what the output of the program looks like all together when selecting the `generic_transactions.csv` as an example."
      ],
      "id": "GfyRXJc_0ZbJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBvPQMnq0ZbJ"
      },
      "outputs": [],
      "source": [
        "Here are the following transactional databases\n",
        " (1) Generic\n",
        " (2) Nike\n",
        " (3) Best Buy\n",
        " (4) Coffee Shop\n",
        " (5) K-mart\n",
        "\n",
        "Enter number to select a database:\n",
        "1\n",
        "=============================================================\n",
        "Here are the unique items corresponding to the transactions:\n",
        "=============================================================\n",
        " Item # Item\n",
        "      1    A\n",
        "      2    B\n",
        "      3    C\n",
        "      4    D\n",
        "      5    E\n",
        "      6    F\n",
        "================================================\n",
        "Enter minimum support (e.g., 0.3 for 30%): 0.3\n",
        "Enter minimum confidence (e.g., 0.6 for 60%): 0.6\n",
        "\n",
        "Using min_support = 0.3 and min_confidence = 0.6\n",
        "\n",
        "\n",
        "Running Brute-Force Algorithm...\n",
        "Found 5 frequent 1-itemsets\n",
        "Found 6 frequent 2-itemsets\n",
        "Found 2 frequent 3-itemsets\n",
        "\n",
        "Running Apriori Algorithm...\n",
        "Running FP-Growth Algorithm...\n",
        "\n",
        "\n",
        "================================================\n",
        "🔹 FREQUENT ITEMS FOUND BY BRUTE FORCE:\n",
        "================================================\n",
        "('A',) | support: 1.00\n",
        "('B',) | support: 0.40\n",
        "('C',) | support: 0.60\n",
        "('D',) | support: 0.45\n",
        "('E',) | support: 0.70\n",
        "('A', 'B') | support: 0.40\n",
        "('A', 'C') | support: 0.60\n",
        "('A', 'D') | support: 0.45\n",
        "('A', 'E') | support: 0.70\n",
        "('C', 'D') | support: 0.30\n",
        "('C', 'E') | support: 0.35\n",
        "('A', 'C', 'D') | support: 0.30\n",
        "('A', 'C', 'E') | support: 0.35\n",
        "\n",
        "\n",
        "================================================\n",
        "🔸 ASSOCIATION RULES — BRUTE FORCE\n",
        "================================================\n",
        "('B',) → ('A',) (support: 0.40, confidence: 1.00)\n",
        "('A',) → ('C',) (support: 0.60, confidence: 0.60)\n",
        "('C',) → ('A',) (support: 0.60, confidence: 1.00)\n",
        "('D',) → ('A',) (support: 0.45, confidence: 1.00)\n",
        "('A',) → ('E',) (support: 0.70, confidence: 0.70)\n",
        "('E',) → ('A',) (support: 0.70, confidence: 1.00)\n",
        "('D',) → ('C',) (support: 0.30, confidence: 0.67)\n",
        "('D',) → ('C', 'A') (support: 0.30, confidence: 0.67)\n",
        "('A', 'D') → ('C',) (support: 0.30, confidence: 0.67)\n",
        "('C', 'D') → ('A',) (support: 0.30, confidence: 1.00)\n",
        "('C', 'E') → ('A',) (support: 0.35, confidence: 1.00)\n",
        "\n",
        "\n",
        "================================================\n",
        "🔸 ASSOCIATION RULES — APRIORI\n",
        "================================================\n",
        "('B',) → ('A',) (support: 0.40, confidence: 1.00)\n",
        "('C',) → ('A',) (support: 0.60, confidence: 1.00)\n",
        "('A',) → ('C',) (support: 0.60, confidence: 0.60)\n",
        "('D',) → ('A',) (support: 0.45, confidence: 1.00)\n",
        "('E',) → ('A',) (support: 0.70, confidence: 1.00)\n",
        "('A',) → ('E',) (support: 0.70, confidence: 0.70)\n",
        "('D',) → ('C',) (support: 0.30, confidence: 0.67)\n",
        "('D', 'C') → ('A',) (support: 0.30, confidence: 1.00)\n",
        "('D', 'A') → ('C',) (support: 0.30, confidence: 0.67)\n",
        "('D',) → ('C', 'A') (support: 0.30, confidence: 0.67)\n",
        "('E', 'C') → ('A',) (support: 0.35, confidence: 1.00)\n",
        "\n",
        "\n",
        "================================================\n",
        "🔸 ASSOCIATION RULES — FP-GROWTH\n",
        "================================================\n",
        "('C',) → ('A',) (support: 0.60, confidence: 1.00)\n",
        "('A',) → ('C',) (support: 0.60, confidence: 0.60)\n",
        "('E', 'C') → ('A',) (support: 0.35, confidence: 1.00)\n",
        "('B',) → ('A',) (support: 0.40, confidence: 1.00)\n",
        "('D',) → ('A',) (support: 0.45, confidence: 1.00)\n",
        "('D',) → ('C',) (support: 0.30, confidence: 0.67)\n",
        "('D', 'C') → ('A',) (support: 0.30, confidence: 1.00)\n",
        "('D', 'A') → ('C',) (support: 0.30, confidence: 0.67)\n",
        "('D',) → ('C', 'A') (support: 0.30, confidence: 0.67)\n",
        "('E',) → ('A',) (support: 0.70, confidence: 1.00)\n",
        "('A',) → ('E',) (support: 0.70, confidence: 0.70)\n",
        "\n",
        "\n",
        "================================================\n",
        "⏱️ EXECUTION TIME SUMMARY (seconds)\n",
        "================================================\n",
        "Brute-Force Algorithm: 0.0130 sec\n",
        "Apriori Algorithm:     0.0000 sec\n",
        "FP-Growth Algorithm:   0.0202 sec\n",
        "================================================\n",
        "\n",
        "✅ All algorithms executed successfully!"
      ],
      "id": "JBvPQMnq0ZbJ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBQm2MUR0ZbJ"
      },
      "source": [
        "## Conclusion\n",
        "All three algorithms produced the same frequent itemsets and rules, confirming correctness.\n",
        "- **Brute Force** verified the logic of the libraries.\n",
        "- **Apriori** was efficient for smaller datasets.\n",
        "- **FP-Growth** was the fastest overall.\n",
        "\n"
      ],
      "id": "MBQm2MUR0ZbJ"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}